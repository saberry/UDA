---
title: "Topic Models"
format:
  html:
    toc: true
    toc-location: left
    self-contained: true
---

We already know that text is everywhere and our sentiment analysis was a reasonable crack at extracting some meaning from text. A common line of inquiry relates to what is expressed within the text. In traditionally social sciences, this was done through some form of content analysis -- a coding scheme was developed by researchers, people read each comment and assigned it a code, some agreement statistics were computed, and any discrepancies were hashed out by the researchers. When this was the only option, it was serviceable. 

The amount of information available to us now does not work for those traditional methods. Could you do this for thousands of tweets? With time, maybe. Could you do this with a few million articles? No. This is where topic models come to save the day. Our topic models are going to give us a pretty good idea about what texts are similar to other texts.

For the sake of exploration, it is great to know what topics are *latent* within a body of texts. It also has broader uses. When you search for something, topic models will return a set of documents that are likely to be related to that search. 

## Latent Dirichlet Allocation

Latent Semantic Analysis is the godfather of topic models and non-negative matrix factorization is a generally useful tool (it is great for working with images and has great extensions to factor analysis). For topic models, though, an important tool is latent dirichlet allocation (LDA).

Let's start with a brief demonstration of a standard latent dirichlet allocation (LDA) for topic modeling. A main point to take here is that the main driver of LDA is...the Dirichlet distribution. You can think of the Dirichlet distribution as a multivariate beta distribution (many possible categories, with probabilities of belonging to the category being between 0 and 1).

Suffice it to say, one can approach this in (at least) one of two ways. In one sense, LDA is a dimension reduction technique, much like the family of techniques that includes PCA, factor analysis, non-negative matrix factorization, etc. We will take a whole lot of terms, loosely defined, and boil them down to a few topics. In this sense, LDA is akin to discrete PCA. Another way to think about this is more from the perspective of factor analysis, where we are keenly interested in interpretation of the result, and want to know both what terms are associated with which topics, and what documents are more likely to present which topics. The following is the plate diagram and description for standard LDA from <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">Blei, Jordan, and Ng (2003)</a>.

<aside>
Look at the citation count on that paper!
</aside>

![](bleiPlate.png)

- $\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions
- $\eta$ is the parameter of the Dirichlet prior on the per-topic word distribution
- $\theta_m$ is the topic distribution for document *m*
- $\beta_k$ is the word distribution for topic *k*
- $z_{mn}$ is the topic for the n-th word in document *m*
- $w_{mn}$ is the specific word

Both *z* and *w* are from a multinomial draw based on the $\theta$ and $\beta$ distributions respectively. The key idea is that, to produce a given document, one draws a topic, and given the topic, words are drawn from it.

Here is Blei's classic, short <a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">introduction</a> to probablistic topic models.

### Topic Probabilities

We will start by creating topic probabilities. There will be *k* = 3 topics. Half of our documents will have probabilities of topics for them ($\theta_1$) which will be notably different from the other half ($\theta_2$). Specifically, the first half will show higher probability of topic "A" and "B", while the second half of documents show higher probability of topic "C". What we’ll end up with here is an $m X k$ matrix of probabilities $\theta$ where each *m* document has a non-zero probability for each *k* topic. Note that in the plate diagram, these would come from a Dirichlet($\alpha$) draw rather than be fixed like we are doing, but hopefully this will make things clear starting out.


```{r}
library(tidyverse)

nDocs = 500                                       # Number of documents

wordsPerDoc = rpois(nDocs, 100)                   # Total words/terms in a document

thetaList = list(c(A = .60, B = .25, C = .15),    # Topic proportions for first and second half of data 
                 c(A = .10, B = .10, C = .80))    # These values represent a Dir(alpha) draw

theta_1 = t(replicate(nDocs / 2, thetaList[[1]]))

theta_2 = t(replicate(nDocs / 2, thetaList[[2]]))

theta = rbind(theta_1, theta_2)      
```


### Topic Assignments and Labels

With topic probabilities in hand, we’ll draw topic assignments from a categorical distribution.

```{r}
firsthalf = 1:(nDocs / 2)
secondhalf = (nDocs / 2 + 1):nDocs

Z = apply(theta, 1, rmultinom, n = 1, size = 1)   # draw topic assignment

rowMeans(Z[, firsthalf])                           # roughly equal to theta_1
```

Now, we can see which is the most likely topic.

```{r}
z = apply(Z, 2, which.max) 
```

We have our list of documents and each document's topic assignment. 


### Topics

Next we need the topics themselves. Topics are probability distributions of terms, and in what follows we’ll use the Dirichlet distribution to provide the prior probabilities for the terms. With topic A, we’ll make the first ~40% of terms have a higher probability of occurring, the last ~40% go with topic C, and the middle more associated with topic B. To give a sense of the alpha settings, alpha = c(8, 1, 1) would result in topic probabilities of .8, .1, .1, as would alpha = c(80, 10, 10). We’ll use the gtools package for the rdirichlet function. 

```{r}
nTerms = max(wordsPerDoc)

breaks = quantile(1:nTerms, c(.4,.6,1)) %>% round()

cuts = list(1:breaks[1], (breaks[1] + 1):breaks[2], 
            (breaks[2] + 1):nTerms)

library(gtools)

B_k = matrix(0, ncol = 3, nrow = nTerms)

B_k[,1] = rdirichlet(n=1, alpha=c(rep(10, length(cuts[[1]])),    # topics for 1st 40% of terms
                                  rep(1,  length(cuts[[2]])),
                                  rep(1,  length(cuts[[3]]))))

B_k[,2] = rdirichlet(n=1, alpha=c(rep(1,  length(cuts[[1]])),    # topics for middle 20%
                                  rep(10, length(cuts[[2]])),
                                  rep(1,  length(cuts[[3]]))))

B_k[,3] = rdirichlet(n=1, alpha=c(rep(1,  length(cuts[[1]])),    # topics for last 40%
                                  rep(1,  length(cuts[[2]])),
                                  rep(10, length(cuts[[3]]))))
```


Here is a visualization of the term-topic matrix, where the dark represents terms that are notably less likely to be associated with a particular topic.

```{r}
library(ggplot2)

as.data.frame(B_k) %>%
  mutate(document = 1:nrow(.)) %>% 
  tidyr::gather(key = topic, value = prob, -document) %>% 
  ggplot(., aes(topic, document, color = prob)) +
  geom_tile()
```

Remember how we specified this -- we assigned 40% to topic 1, 40% to topic 3, and left the middle 20% to topic 2. This visualization clearly shows this.

Now, given the topic assignment, we draw words for each document according to its size via a multinomial draw, and with that, we have our document-term matrix. However, we can also think of each document as merely a bag of words, where order, grammar etc. is ignored, but the frequency of term usage is retained.

```{r}
wordlist_1 = sapply(1:nDocs, 
                    function(i) t(rmultinom(1, size = wordsPerDoc[i], prob = B_k[, z[i]])), 
                    simplify = FALSE)  

# smash to doc-term matrix
dtm_1 = do.call(rbind, wordlist_1)

colnames(dtm_1) = paste0('word', 1:nTerms)

# bag of words representation
wordlist_1 = lapply(wordlist_1, function(wds) rep(paste0('word', 1:length(wds)), wds))
```

If you print out the wordlist_1 object, you will see the words asssociated with each document.

## Topic Models

Now with some theory under our belt, we can take a look at analyzing real data.

Just like our sentiment analysis, there is a fair chunk of cleaning to do. The `textProcessor` function would do a lot of work for us, but we don't really need it to do as much since we already created a pretty clean object. We are, however, going to randomly remove some observations from our data (if the reasons are not obvious now, they will be soon). 

We are going to be constructing what are called <a href="https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf">structural topic models</a>.

```{r}
library(dplyr)
library(quanteda)
library(stringr)
library(stm)
library(tm)

all_lyrics_info <- arrow::read_feather("data/billboard_songs_11_23.feather")

model_data <- all_lyrics_info[duplicated(all_lyrics_info$lyric_link) == FALSE, 
                              c("lyrics", "genre", "week")] %>% 
  na.omit()

Encoding(model_data$lyrics) <- "UTF-8"

model_data$lyrics <- gsub(".*\\slyrics\\s", "", model_data$lyrics)

model_data$lyrics <- gsub(
  "see .* live get tickets as low as \\d+", 
  "", 
  model_data$lyrics
)

rm(all_lyrics_info)

model_data$week <- stringr::str_extract(model_data$week, "\\d{4}")

colnames(model_data)[colnames(model_data) == "week"] <- "year"

model_data$lyrics <- model_data$lyrics %>% 
  textclean::replace_contraction() %>% 
  str_replace_all(., "\n", " ") %>% 
  str_replace_all(., "(\\[.*?\\])", "") %>% 
  str_squish() %>% 
  gsub("([a-z])([A-Z])", "\\1 \\2", .) %>% 
  tolower() %>% 
  textstem::lemmatize_strings(.) %>% 
  removeWords(stopwords("SMART"))

# creates the corpus with document variables except for the "text"
lyric_corpus <- corpus(model_data, text_field = "lyrics")

lyric_token <- tokens(lyric_corpus, 
                      remove_punct = TRUE, 
                      remove_symbols = TRUE,
                      remove_numbers = TRUE)

lyric_dfm <- dfm(lyric_token)

lyric_dfm <- dfm_trim(lyric_dfm, sparsity = 0.990)

lyric_stm <- convert(lyric_dfm, to = "stm")

docs_stm <- lyric_stm$documents 
vocab_stm <- lyric_stm$vocab    
meta_stm <- lyric_stm$meta

lyricPrep <- prepDocuments(documents = docs_stm, 
                           vocab = vocab_stm,
                           meta = meta_stm)

save(lyricPrep, file = "data/lyricPrep_11_24.RData")
```

The stm package has some pretty nice facilities for determining a number of topics. Be warned, though, that this takes a long time to run. Mac users can use the `cores` argument, but Windows users will need to do something a little craftier.

```{r, eval=FALSE}
kTest <- searchK(documents = lyricPrep$documents, 
             vocab = lyricPrep$vocab, 
             K = c(3, 4, 5, 10, 20), verbose = FALSE)

plot(kTest)
```

```{r, echo = FALSE}
if(Sys.info()["sysname"] == "Darwin") {
  load("~/courses/unstructured/data/kTest.RData")
} else load("D:/projects/UDA/data/ktest.RData")

plot(kTest)
```

The 4 plots that are returned are going to try to help us determine the best number of topics to take. I like to focus on semantic coherence (how well the words hang together -- computed from taking a conditional probability score from frequent words) and the residuals. We want to have low residual and high semantic coherence. The residuals definitely take a sharp dive as we increase K. If we consider one of the major assumptions of LDA, we could almost always guess that we would need a great number of topics (i.e., if every topic that has ever existed, existed before writing, then we could have a huge number of topics). Our coherence seems to do pretty well with some of the lower numbers (not entirely surprising); coherence is essentially a measure of human interpretability. With all of these together, we can settle on 5 topics for our subsequent analyses. Just for your information, held-out likelihood can be characterized by how surprised the model would be with new words (it is often called perplexity). Lower bound relates to the lower bound of the marginal likelihood.

It is worthwhile to note that we can also do model selection with the stm package, but that is some work that will be best done if you want some additional playtime.

With our 5 topics, we can start our actual model:

```{r}
load("data/lyricPrep_11_24.RData")
topics5 <- stm(documents = lyricPrep$documents, 
             vocab = lyricPrep$vocab, seed = 1001,
             K = 5, verbose = FALSE)
```

We can get a lot of output from our model, but we can focus on the expected topic proportions plot:

```{r}
plot(topics5)
```

We are essentially looking at the proportion of each topic, with a few of the highest probability words. 

This is great, but it is really fun to see what emerges from the topics. 

```{r}
labelTopics(topics5)
```

Let's focus on the frex words (they occur *fr*equently within the topic and are *ex*clusive to that topic) and the highest probability words (i.e., the words that have the highest probability of occurring within that topic). The Lift (calculated by dividing by frequencies in other topics) and Score (uses log frequencies) words can be useful, but are a bit less intuitive than the other two -- they are really there to give an idea about how common the words are in comparison to how common they are in other topics. Let's put some names to the topics. 

We can look at lyrics that have a high probability of being associated with each topic:

```{r}
findThoughts(topics5, texts = lyricPrep$meta$text, n = 1)
```

We can see that the story we put to our topics makes sense, given what we see in the actual texts.

One of the great things about topic models is that they are probabilistic, meaning that each document has a certain probability of belonging to a topic:

```{r}
head(topics5$theta, 15)
```

Document 1, for example has a probability of ~.19 for belonging to topic 1 and .78 for belonging to topic 2. Document 3 has a bit of a range, with probabilities of .31, .20, and .42 for belonging to topics 1, 2, and 3.

```{r}
lyricPrep$meta[1, ]

lyricPrep$meta[3, ]
```

We can also see what terms are in documents 1:

```{r}
lyricPrep$documents[[1]]
```


```{r}
lyricPrep$vocab[lyricPrep$documents[[1]][1, ]]
```

And 3:

```{r}
lyricPrep$documents[[3]]
```

```{r}
lyricPrep$vocab[lyricPrep$documents[[3]][1, ]]
```

```{r}
topics5_genre <- stm(documents = lyricPrep$documents, 
             vocab = lyricPrep$vocab, 
             content = ~genre, data = lyricPrep$meta,
             seed = 1001,
             K = 5, verbose = FALSE)

plot(topics5_genre, type = "perspectives", topics = 1)
```


Let's see how these work in a model.

```{r}
library(mlr3)
library(mlr3learners)

dfm_df <- convert(lyric_dfm, to = "data.frame")

model_data <- as.data.frame(cbind(topics5$theta, meta_stm$genre))

colnames(model_data) <- c(paste0("topic_", 1:5), "genre")

topic_cols <- which(grepl("topic", colnames(model_data)))

model_data[, topic_cols] <- apply(model_data[, topic_cols], 2, as.numeric)

model_data$genre <- as.factor(model_data$genre)

# create learning task
music_task <- TaskClassif$new(
  id = "lyrics", backend = model_data, target = "genre"
)
music_task

train_set <- sample(music_task$nrow, 0.8 * music_task$nrow)
test_set <- setdiff(seq_len(music_task$nrow), train_set)


mlr_learners$get("classif.xgboost")

learner <- lrn("classif.xgboost", nthread = 7, eta = .1, nrounds = 25)

learner$train(music_task, row_ids = train_set)

prediction <- learner$predict(music_task, row_ids = test_set)

prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)

```

Topic models can be extended to include covariates, where we are examining how much each topic contributes to a document given some other data.

This is where some real explanatory power comes into play!

```{r}
if(Sys.info()["sysname"] == "Darwin") {
  load("~/courses/unstructured/data/allLyricsDF.RData")
} else load("D:/projects/courses/unstructured/data/allLyricsDF.RData")

all_lyrics <- arrow::read_feather("data/country_only_70_24.feather")

all_lyrics$date <- stringr::str_extract(all_lyrics$og, "\\d{4}")

all_lyrics$date <- as.numeric(all_lyrics$date)

all_lyrics$text <- textstem::lemmatize_strings(all_lyrics$lyrics)

predictorText <- textProcessor(documents = all_lyrics$text, 
                          metadata = all_lyrics, 
                          stem = FALSE)

lyricPrep <- prepDocuments(documents = predictorText$documents, 
                               vocab = predictorText$vocab,
                               meta = predictorText$meta)

topicPredictor <- stm(documents = lyricPrep$documents,
             vocab = lyricPrep$vocab, prevalence = ~ date,
             data = lyricPrep$meta, K = 5, verbose = FALSE)

yearEffect <- estimateEffect(1:5 ~ date, stmobj = topicPredictor,
               metadata = lyricPrep$meta)
save(topicPredictor, yearEffect, file = "data/country_only_stm.RData")
```

```{r}
load("data/country_only_stm.RData")
labelTopics(topicPredictor)

summary(yearEffect, topics = c(1:5))

plot.estimateEffect(x = yearEffect, covariate = "date", method = "continuous",
                    model = topicPredictor, topics = 1, labeltype = "frex")

plot.estimateEffect(yearEffect, "date", method = "continuous",
                    model = topicPredictor, topics = 2, labeltype = "frex")

plot.estimateEffect(yearEffect, "date", method = "continuous",
                    model = topicPredictor, topics = 3, labeltype = "frex")

plot.estimateEffect(yearEffect, "date", method = "continuous",
                    model = topicPredictor, topics = 4, labeltype = "frex")

plot.estimateEffect(yearEffect, "date", method = "continuous",
                    model = topicPredictor, topics = 5, labeltype = "frex")
```


## Topic Models and Transformer

The greatest bulk of this work can be done simply with BertTopic:

```{python}
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
import pandas as pd

## Vanilla ##

songs = pd.read_feather(
  'C:/Users/sberry5/Documents/teaching/UDA/data/all_lyrics.feather'
)

songs = songs.dropna()

songs['lyrics'] = songs['lyrics'].astype('str')

ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)

topic_model = BERTopic(ctfidf_model=ctfidf_model)

topics, probs = topic_model.fit_transform(songs['lyrics'].to_list())

topic_model.get_topic_info()

topic_model.get_topic(0)

topic_model.get_document_info(songs['lyrics'])

topic_model.get_representative_docs(0)

topic_model.generate_topic_labels()

topic_model.reduce_topics(songs['lyrics'].to_list(), nr_topics=10)

## Per Class ##

docs = songs['lyrics'].to_list()
targets = songs['genre'].to_list()

topics_per_class = topic_model.topics_per_class(docs, classes=targets)

topic_model.visualize_topics_per_class(topics_per_class, top_n_topics=10)
```

