---
title: "Text Analysis"
description: |
  Introductory Concepts
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


The ability to produce written word is one of the major separators between us and the other great apes. And with the ability to produce it, comes the benefit of interpreting it.

## Processing Text

Before we can even begin to dive into analyzing text, we must first process the text. Processing text involves several steps that will be combined in various ways, depending on what we are trying to accomplish.

### Stemming

Tense aside, are loved, love, and loving the same thing? Yes, but what if we compare the actual strings? On a string comparison side, are they the same? No. We have a string with 6, 4, and 7 characters, respectively.

What if we remove the suffixes, "ed" and "ing" -- we are left with three instances of "love"? Now we have something that is equivalent in meaning and in a string sense. This is the goal of stemming.  

Let's take a look to see how this works (you will need to install `tm` and `SnowballC` first):

```{r}
love_strings <- c("love", "loving", "loved", "lover")

tm::stemDocument(love_strings)
```

We got exactly what we expected, right? You might have noticed that "chewer" did not get stemmed. Do you have any idea why? Let's think through it together. "Chew", "chewing", and "chewed" are all verbs related to the act of chewinging. "Chewer", on the other hand, is a person who chews -- it is a noun. Martin Porter's stemming <a href="http://cs.indstate.edu/~skatam/paper.pdf">algorithm</a> works incredibly well!

Hopefully, this makes conceptual sense; however, we also need to understand why we need to do it. In a great many text-based methods, we are going to create a matrix that keeps track of every term (i.e., word) in every document -- this is known as a document-term matrix. If we know that "chew", "chewing", and "chewed" all refer to the same thing, we want it just represented once within our document-term matrix.

Shall we take a look?

```{r}
library(tm)

documents <- c("I've loved you three summers now, honey, but I want them all", 
              "You're my, my, my, my lover", 
              "Tell me, tell me if you love me or not", 
              "Tryna do what lovers do, ooh", 
              "I just, I can't, I just canâ€™t be loving you no more")

documentsCorp <- tm::SimpleCorpus(VectorSource(documents))

documentsDTM <- DocumentTermMatrix(documentsCorp)

inspect(documentsDTM)
```

We can see that without stemming, we have 9 terms (things like "I", "a", and "to" get removed automatically). Let's do some stemming now:

```{r}
documentsStemmed <- stemDocument(documents)

documentsStemmed
```

And now the document-term matrix:

```{r}
stemmedDocCorp <- tm::SimpleCorpus(VectorSource(documentsStemmed))

stemmedDocDTM <- DocumentTermMatrix(stemmedDocCorp)

inspect(stemmedDocDTM)
```

If we are trying to find documents that are covering similar content or talking about similar things, this document-term matrix will help to draw better conclusions, because it is clear that the first three documents are talking about the act of chewing and this document-term matrix reflects that.

### Lemmatization

Stemming is often sufficient (and most modern stemmers work pretty well on their own). Still, stemming is slightly more akin to amputating an arm with a battle ax -- it works, but it is brute force. Lemmatization is a more sophisticated approach. You might have already guessed that lemmatization will find the *lemma* of a word and since you likely know about morphology, you already know that the lemma of a word is its canonical form. A group of words that form the same idea are called a lexeme (am, be, are are all within the same lexeme). Generally, the smallest form of the word is chosen as the lemma. This is a really interesting area of linguistics, but we don't need to dive fully in.  

Instead, let's see it in action.

If we compare some "chewing" stuff on stemming and lemmatization, we can see what we get:

```{r}
library(textstem)

love_string <- c("love", "loving", "loved", "lover")

stem_words(love_string)

lemmatize_words(love_string)
```

Absolutely nothing different. Both stemming and lemmatizing will perform the same task. The act of chewing is comprised of a past, present, and future tense, and chew is the lemma; chewer is still seen as something else entirely.

But let's take a look at something different. If we have a string of the most lovely words, what might happen?

```{r}
lovely_string <- c("lovely", "lovelier", "loveliest")

stem_words(lovely_string)
```

That is about as close to "bigly" nonsense as we could possibly get without going into Dr. Suess mode. 

But if we try lemmatization:

```{r}
lemmatize_words(lovely_string)
```

We get something that starts to make sense. Now, let's try these on some actual chunks of text and see what happens.

```{r}
# This data is in the "data" folder on Sakai!

load("D:/projects/courses/unstructured/data/allLyricsDF.RData")

sampleLyrics <- allLyricsDF[40, ]

sampleLyrics$lyrics
```

Of course, we will need to do some cleaning on our text first:

<aside>
As we work with this lyric stuff, you will need to make sure that you clean it before playing with it. The code that we saw last week should take care of it...maybe!
</aside>

```{r}
library(dplyr)

library(stringr)

cleanLyrics <- sampleLyrics$lyrics %>% 
  str_replace_all(., "\n", " ") %>% 
  str_replace_all(., "\\[[A-Za-z]+\\s*[0-9]*]", "") %>%
  str_squish(.) %>% 
  gsub("([a-z])([A-Z])", "\\1 \\2", .)
```

We have to try the obligatory stemming:

```{r}
stem_strings(cleanLyrics)
```

And now the lemmatized version:

```{r}
lemmatize_strings(cleanLyrics)
```

Here is something very interesting:

```{r}
microbenchmark::microbenchmark(stem_strings(cleanLyrics), 
                               lemmatize_strings(cleanLyrics))
```

What is the point here? This song has a little over 400 words in it. Stemming, over 100 runs, took on average 1.3 milliseconds, while lemmatizing took 3.6. We are just talking milliseconds, so this is almost to the point where we would not notice; but, if we did this over an entire corpus, we would definitely notice the time.

```{r}
lemmatize_strings(documents)
```


The question, then, is what do you decide to do. For my money, lemmatization does a better job and getting words down to their actual meaning.

```{python}
from nltk.stem import WordNetLemmatizer
import pyreadr

# You will only need to do this once:

# nltk.download('wordnet')
# nltk.download('omw-1.4')

# Obs -- you'll need to import nltk.

lyrics = pyreadr.read_r("D:/projects/courses/unstructured/data/allLyricsDF.RData")

lyrics_pd = pd.DataFrame(lyrics['allLyricsDF'])

lyrics_sample = lyrics_pd.sample(1)

w_tokenizer = nltk.tokenize.WhitespaceTokenizer()
lemmatizer = nltk.stem.WordNetLemmatizer()

def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]

lyrics_sample['lyric_lemma'] = lyrics_sample.lyrics.apply(lemmatize_text)
```


### Stop Words

Some words do us very little good: articles, prepositions, and very high frequency words. These are all words that need to be removed. Fortunately, you don't have to do this on your own -- a great many dictionaries exist that contain words ready for removal.

```{r}
tm::stopwords("en")
```

Removing stopwords takes little effort!

```{r}
tm::removeWords(tolower(documents), words = stopwords("en"))
```

We can even include custom stopwords:

```{r}
tm::removeWords(tolower(documents), words = c("tryna", stopwords("en")))
```

There are many different stopword lists out there, so you might want to poke around just a little bit to find something that will suit the needs of a particular project. 

```{r}
library(stopwords)
```

Applied to our previous song, here is what we would get:

```{r}
tm::removeWords(cleanLyrics, words = stopwords("en"))
```

Now, let's use the `textclean` package to handle contraction replacement:

```{r}
replacedText <- textclean::replace_contraction(cleanLyrics)

tm::removeWords(replacedText, words = stopwords("en"))
```

There are several great functions in `textclean` -- I highly suggest you check it out.

And one final point to make:

```{r}
gsub('"', "", replacedText)
```


## Text Processing Tools

There are several R packages that will help us process text. The tm package is popular and automates most of our work. You already saw how we use the stemming and stopword removal functions, but tm is full of fun stuff and allows for one pass text processing.

```{r, eval = FALSE}
documentCorp <- SimpleCorpus(VectorSource(documents))

stopWordRemoval <- function(x) {
  removeWords(x, stopwords("en"))
}

textPrepFunctions <- list(tolower,
                         removePunctuation,
                         lemmatize_strings,
                         stopWordRemoval,
                         removeNumbers,
                         stripWhitespace)

documentCorp <- tm_map(documentCorp, FUN = tm_reduce, tmFuns = textPrepFunctions)

documentCorp[1][[1]]$content
```

Once you get your text tidied up (or even before), you can produce some visualizations!

```{r}
library(tidytext)

library(wordcloud2)

allLyricsDF %>%
  dplyr::filter(stringDistance < .2) %>% 
  dplyr::select(lyrics, returnedArtistName) %>%
  mutate(lyrics = as.character(lyrics), 
         lyrics = str_replace_all(lyrics, "\n", " "),   
         lyrics = str_replace_all(lyrics, "\\[[A-Za-z]+\\s*[0-9]*]", ""), 
         lyrics = str_squish(lyrics), 
         lyrics = gsub("([a-z])([A-Z])", "\\1 \\2", lyrics)) %>%
  unnest_tokens(word, lyrics) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 25) %>% 
  na.omit() %>% 
  wordcloud2(shape = "cardioid")
```

# Initial Analyses

Like every analysis you will ever do, it is easy to try jumping right into the most complex questions you can answer with text -- and it is never the right thing to do. Text gives us the ability to do a lot of exploratory data analysis, so let's start there.

Let's start by finding a little bit of text. There is a lot out there, but let's grab some "interesting" song lyrics. 

```{r}
library(rvest)

library(stringr)

hflLyrics <- read_html("https://genius.com/Luke-bryan-huntin-fishin-and-lovin-every-day-lyrics") %>% 
  html_nodes("#lyrics-root") %>% 
  html_text()

hflLyrics
```

We can see that we have the data, but we are left with a complete mess.

```{r}
hflLyrics <- str_replace_all(hflLyrics, "\n", " ") %>% 
  str_replace_all(., "\\[[A-Za-z]+\\s*[0-9]*]", "") %>%
  str_squish(.) %>% 
  gsub("([a-z])([A-Z])", "\\1 \\2", .)

hflLyrics
```

And there you have a #1 Country Song from just a few years ago.

```{python}
from bs4 import BeautifulSoup
import pandas as pd
import requests

hfl_request = requests.get('https://genius.com/Luke-bryan-huntin-fishin-and-lovin-every-day-lyrics')

hfl_content = BeautifulSoup(hfl_request.content, 'html.parser') 

hfl_lyrics = hfl_content.select('#lyrics-root')

hfl_list = []

for i in range(len(hfl_lyrics)):
    hfl_list.append(hfl_lyrics[i].getText())

hfl_pd = pd.DataFrame([hfl_list], columns = ['lyrics'])

hfl_pd['lyrics'] = (
  hfl_pd.lyrics.str.replace('\\[[A-Za-z]+\\s*[0-9]*]', '') 
  .str.replace('([a-z])([A-Z])', '\\1 \\2') 
)


```


For those that might like a little more grit to their Country, let's look at another song:

```{r}
copperheadLyrics <- read_html("https://genius.com/Steve-earle-copperhead-road-lyrics") %>% 
  html_nodes("#lyrics-root") %>% 
  html_text() %>% 
  str_replace_all(., "\n", " ") %>% 
  str_replace_all(., "\\[\\w+\\s*[0-9]*]", "") %>% 
  str_squish(.) %>% 
  gsub("([a-z])([A-Z])", "\\1 \\2", .)

copperheadLyrics
```

And here is some more underground country:

```{r}
choctawBingoLyrics <- read_html("https://genius.com/James-mcmurtry-choctaw-bingo-lyrics") %>% 
  html_nodes("#lyrics-root") %>% 
  html_text() %>% 
  str_replace_all(., "\n", " ") %>% 
  str_replace_all(., "\\[\\w+\\s*\\w*\\]", "") %>% 
  str_squish(.) %>% 
  gsub("([a-z])([A-Z])", "\\1 \\2", .)

choctawBingoLyrics
```

<aside>
You will notice that we ran the same code three times -- probably time to create a function.
</aside>

We clearly have very different songs: one about living the outlaw life, one about living the "country-bro" life, and one about your typical American family reunion. From here on, it might be worth exploring more about these three types of songs.

## Term Frequency

Just like any other data, text has some basic descriptives, with term frequency (tf -- $f_{t,d}$) being incredibly useful. When we are looking at term frequency, we are looking for a few different words: high and low frequency. If a word is high frequency (think: "the"), then it might not really be offering us much in the way of anything informative. Likewise, a word that only occurs once or twice might not be terribly important either. 

We can calculate term frequency (adjusted for for document length) as the following:

$$tf=\frac{N_{term}}{Total_{terms}}$$

When looking at a corpus, it is important to adjust for the length of the text when calculating term frequency (naturally, longer texts will have words occurring more frequently). 

There are a few other ways of calculating term frequency:

A raw weight is depicted as $f_{t,d}$ -- the frequency with which *t* (the term) is found in *d* (the document)

If you want to effectively normalize huge numbers and minimize the differences between huge numbers, $log(1+f_{t,d})$

If you have huge differences in document length, you might use augmented term frequency: $k + (1-k)\frac{tf}{max(t,f)}$, where *k* helps to mitigate the effects of document length (it essentially removes the bias towards longer documents).

This <a href="https://arxiv.org/ftp/arxiv/papers/1604/1604.04007.pdf">article</a> has a nice run down of more methods.

Let's see what we have in the way of term frequency in our songs:

```{r}
library(dplyr)

library(tidytext)

songData <- data.frame(song = c("hfl", "copperhead", "bingo"), 
                      lyrics = c(tolower(hflLyrics), tolower(copperheadLyrics), 
                                 tolower(choctawBingoLyrics)), 
                      stringsAsFactors = FALSE)

songTF <- songData %>% 
  split(., .$song) %>%
  lapply(., function(x) {
    songTokens = tm::MC_tokenizer(x$lyrics)
    tokenCount = as.data.frame(summary(as.factor(songTokens), maxsum = 1000))
    total = length(songTokens)
    tokenCount = data.frame(count = tokenCount[[1]], 
                            word = row.names(tokenCount),
                            total = total,
                            song = x$song,
                            row.names = NULL)
    return(tokenCount)
    }) 

songTF <- do.call("rbind", songTF)  

rmarkdown::paged_table(songTF)
  
```


We can sort our raw frequencies in many different ways, but we see some very common words across our documents (the, i, a). Those likely are not important for our understanding of the lyrics (maybe "I", but we can get into the story-telling in a bit).

Let's now take our frequencies and divide by the number of terms within the document:

```{r}
songTF$tf <- songTF$count/songTF$total

rmarkdown::paged_table(songTF)
```

This provides a nice term frequency adjusted for the length of the document. There are others (e.g., log scaling, normalized, and double normalized), but this clearly-adjusted term frequency will more than suit our needs here. If we had documents of wildly-different lengths, we would explore some alternatives. 


## Inverse Document Frequency

We can know how many times any word was used within a text when we look at our term frequencies. Inverse document frequency (IDF) gives us something a little bit different. If a word is incredibly common, it might not be very important to a document; however, rare words might be important within our documents. To that end, we would assign a higher weight to words that occur less frequently than words that are common.

We can calculate idf as the natural log of the number of the number of documents divided by the number of documents containing the term. We really don't need any fancy functions to make that calculation -- we can just do it by hand in a mutate function.

```{r}
idfDF <- songTF %>% 
  group_by(word) %>% 
  count() %>% 
  mutate(idf = log((length(unique(songTF$song)) / n)))

rmarkdown::paged_table(idfDF)
```

Our idf is just telling us what we need to know about the corpus-wide term counts. We can see that words that appear in all three of our songs have a very low idf, while words that appear in only song have a much higher idf.

## tf-idf

After considering the two in isolation, we can also consider what both of them will get for us together. If we take the term frequency to mean that words are appearing frequently within our text and we take our inverse document frequency to mean that we are only considering important words, we might imagine a set of words appearing commonly within a document, but not appearing within other documents as often. This would suggest high-weight words for a specific document. 

It can be tempting to just cut stop words (which we will discuss next class) out and deal with everything that comes out -- this is not the place for that. Stopword removal, for all practical purposes, is brute force. If we want to have a bit of finese here, we want to leave open the possibility that words, even potentially common words within a document, can have different levels of importance across documents.

To get our tf-idf, let's join our tf data and our idf data together:

```{r}
tfidfData <- merge(songTF, idfDF, by = "word")
```


And from there, it is just simple multiplication between our tf and our idf:

```{r}
tfidfData$tfIDF <- tfidfData$tf * tfidfData$idf

rmarkdown::paged_table(tfidfData)
```

Let's take a look at our top 15 words for each song:

```{r}
tfidfData %>% 
  group_by(song) %>% 
  arrange(song, desc(tfIDF)) %>% 
  slice(1:15) %>% 
  rmarkdown::paged_table()
```

Pretty interesting, right? What is the story here, though? I might be tempted to say that one of these songs is incredibly formulaic, while the other two took a little bit of actual wordsmithing to write.

```{r}
corpus <- Corpus(VectorSource(songData$lyrics))

song_dtm <- DocumentTermMatrix(corpus,
                               control = list(weighting = function(x) {
                                 weightTfIdf(x, normalize = TRUE)},
                                 tolower = TRUE,
                                 removePunctuation = TRUE,
                                 removeNumbers = TRUE,
                                 stopwords = TRUE,
                                 stemming = function(x) lemmatize_strings(x)
                                 ))

inspect(song_dtm)
```


Let's see how we can get tf-idf in Python:

```{python}
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vec = TfidfVectorizer()

hfl_tfidf = tfidf_vec.fit_transform(hfl_pd['lyrics'])

tfidf_tokens = tfidf_vec.get_feature_names_out()

df_countvect = pd.DataFrame(data = hfl_tfidf.toarray(), index = ['hfl'], 
  columns = tfidf_tokens)
```


### Practice Time

Let's take about 20 minutes to work through the various those text descriptives. Just so that we can keep compounding everything, let's scrape some text from wikipedia. Since these will follow a pretty basic construction, grab one more page from Wikipedia (whatever you want).

```{r, eval = FALSE}
library(rvest)

f13Link <- "https://en.wikipedia.org/wiki/Friday_the_13th_(franchise)"

halloweenLink <- "https://en.wikipedia.org/wiki/Halloween_(franchise)"

evilDeadLink <- "https://en.wikipedia.org/wiki/Evil_Dead"

read_html() %>% # Links will go here
  html_nodes() %>% # The p tag will work for getting page text
  html_text() %>% 
  paste0(., collapse = " ") %>% # Here we collapse everything into one string 
  tolower() # Making the words consistent
```

## N-grams

We have primarily been living in the space of single words (and if those single words appear alongside other single words in a text). We know, however, that words rarely appear in isolation -- this is where n-grams come into play. In this case, the *n* is any number that we want to use. In most text exploration, bigrams and trigrams are going to be the most common n-grams that you will use, but you could use anything.

Poking fun at bad country is a good time, so let's keep exploring our previously gotten album:

```{r}
bigrams <- killLightsLyrics %>% 
  unnest_tokens(., ngrams, lyrics, token = "ngrams", n = 2) %>% 
  tidyr::separate(ngrams, c("word1", "word2"), sep = "\\s") %>% 
  count(word1, word2, sort = TRUE)

rmarkdown::paged_table(bigrams)
```

## String Distance And Similarity

Now that we know a little bit about n-grams (and individual words), we can talk about string distances. If you ever need to know how similar (or dissimilar) two words/texts are, then string distances are what you need. But...which one should we use. 

### Levenshtein

This is probably the most common string distance metric you will see (it is pretty common in genetics research, among other areas). Conceptually, it is pretty easy -- we are just finding the number of changes that need to be made to one string to equal another string.

Let's look at two names:

```{r}
library(stringdist)

stringdist("bono", "gaga", method = "lv")
```

To transform "bono" into "gaga", we would need to replace the "b" with a "g", the "o" with an "a", the "n" with a "g", and the "o" with an "a" -- all leading to a Levenshtein distance of 4. We can also look at the similarity between the two:

```{r}
stringsim("bono", "gaga", method = "lv")
```

As to be expected. The similarity is computed as the string distance score, divide it by the maximum feasible distance, and then subtract from 1. 

Those are clearly different words, but what about something a little closer together?

```{r}
stringdist("beauty", "beautiful", method = "lv")
```

Still 4. That is the tricky thing with Levenshtein distance -- string length matters.  

Let's check the similarity now:

```{r}
stringsim("beauty", "beautiful", method = "lv")
```

We have our distance (4), divided by the max possible distance (beautiful has 9 letters, so 4 / 9 = .4444444), and subtract that from 1 (1 - .4444444 = .5555556). 


The similarity here is a bit more telling than our distance. 

### Jaccard

The Jaccard Index is an extremely flexible metric that goes even beyond strings (it is used in computer vision, pure mathematics, and various other places). It is most useful when comparing sets as opposed to just words. 

```{r}
stringdist("soup can", "soup tin", method = "jaccard", q = 1)
```

We can also try it with different values of *q* to really get a feel for what is happening:

```{r}
stringdist("soup can", "soup tin", method = "jaccard", q = 2)
```

Why did our distance increase? Let's break our individual strings into bigrams:

```{r}
can <- paste(unlist(NLP::ngrams(unlist(strsplit("soup can", "")), 2)), collapse = "")

tin <- paste(unlist(NLP::ngrams(unlist(strsplit("soup tin", "")), 2)), collapse = "")

substring(can, seq(1, nchar(can) - 1, 2), seq(2, nchar(can), 2))

substring(tin, seq(1, nchar(tin) - 1, 2), seq(2, nchar(tin), 2))
```

We see that we have 7 bigrams for our vectors. 

```{r}
stringdist("taco", "tako", method = "qgram", q = 2)

stringsim("taco", "tako", method = "qgram", q = 2)
```


```{python}
import textdistance

# install as TextDistance

textdistance.jaccard.distance('soup can', 'soup tin')
textdistance.jaccard.similarity('soup can', 'soup tin')

textdistance.cosine.similarity('soup can', 'soup tin')

textdistance.levenshtein('soup can', 'soup tin')
```

No matter the language you can also use the Jaro-Winkler metric. It gives provides a "score bump" for strings that start with the same letters.

```{python}
textdistance.jaro_winkler.distance('soup can', 'soup tin')
textdistance.jaro_winkler.similarity('soup can', 'soup tin')
```



String distances can be handy for a great many tasks. If you want to find strings that are close to other strings (without being exact matches), then these distances can be useful. They can also be helpful when you want to join data frames with fields that might not match.  

Let's consider the following:

```{r}
library(fuzzyjoin)

companyData <- data.frame(name = c("Pepsi Co.", "PepsiCo", 
                                   "Morgan Stanly", "Morgan Stanley"), 
                          dollars = c(100, 10, 200, 20))

companyData2 <- data.frame(name = c("Pepsi", "Pepsi Co", 
                                    "Morgin Stanley", "Morgan Stanley "), 
                           people = c("Bill", "Bill", "Sue", "Sue"))
```

Before we try to join these data frames, I want to introduce you to my favorite string distance measures: <a href="https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance">Jaro Winkler</a>. It gives a little more granularity than what some of the other metrics might.

```{r}
stringdist("taco", "tako", method = "jw")

stringsim("taco", "tako", method = "jw")
```

```{r}
stringdist_left_join(companyData, companyData, by = "name", 
                     method = "jw", distance_col = "distance")
```

That is a little excessive, so we can specify our `max_dist` to help pull some of those matches back:

```{r}
stringdist_left_join(companyData, companyData, by = "name", 
                     method = "jw", distance_col = "distance", 
                     max_dist = .1)
```

Take warning, though, that this is not a magic bullet and you might need to do some exploring and further cleaning.

## Correlation

Without looking ahead, what is likely the most common measure of association for two binary variables...$\phi$. Conceptually, phi is interpretted just like a Pearson correlation, so we can get a pretty good idea about words co-occuring together. We can calculate phi coefficients for words across documents. Here are all of the elements that we will need:

```{r, echo = FALSE}
library(tibble)

tibble::tibble(` ` = c("Has word X", "No word X", "Total"),
  `Has word Y` = c("n11", "n01", "n.1"),
                   `No word Y` = c("n10", "n00", "n.0"), 
                   Total = c("n1.", "no.", "n"), ) %>% 
  knitr::kable()
```

And we find the correlation with the following:

$$\phi = \frac{n_{11}n_{00} - n_{10}n_{01}}{\sqrt{n_1. n_0. n._0 n._1}} $$

```{r, echo = FALSE}
tibble::tibble(` ` = c("Has word X", "No word X", "Total"),
  `Has word Y` = c("n11(100)", "n01(25)", "n.1(125)"),
                   `No word Y` = c("n10(150)", "n00(40)", "n.0(190)"), 
                   Total = c("n1.(250)", "no.(65)", "n(315)"), ) %>% 
  knitr::kable()
```


```{r}
phi <- ((100 * 40) - (150 * 25)) / sqrt(250 * 65 * 190 * 125)

phi
```

That is a pretty small correlation, let's see a pattern that might result in something a bit more substantial:

```{r, echo = FALSE}
tibble::tibble(` ` = c("Has word X", "No word X", "Total"),
  `Has word Y` = c("n11(150)", "n01(10)", "n.1(160)"),
                   `No word Y` = c("n10(20)", "n00(20)", "n.0(40)"), 
                   Total = c("n1.(170)", "no.(30)", "n(200)"), ) %>% 
  knitr::kable()
```


```{r}
phi <- ((150 * 20) - (20 * 10)) / sqrt(170 * 30 * 40 * 160)

phi
```

Since we already know how we can tokenize our documents using non-tidy work, let's make things a little bit easier for ourselves and use the `widyr` and `tidytext` packages. 

```{r}
library(tidytext)

library(widyr)

songDataCor <- unnest_tokens(songData, words, lyrics)

songDataCor <- songDataCor[!(songDataCor$words %in% stop_words$word), ]

songDataCor %>% 
  group_by(words) %>% 
  filter(n() > 5) %>% 
  pairwise_cor(., words, song, sort = TRUE) %>% 
  rmarkdown::paged_table()
```

With relatively few documents, we don't really have too much interesting to explore here. We could, though, try something a little more interesting (and likely predictable). 

Since we already saw some of the best that modern country has to offer, let's look at an entire album worth of country music (Luke Bryan should offer some predictable results). We can scrape the song lyrics from the Genius page.

```{r, echo = FALSE}
load("/Users/sethberry/courses/unstructured/data/killLightsLyrics.RData")
```


```{r, eval = FALSE}
killTheLightsLink <- read_html("https://genius.com/albums/Luke-bryan/Kill-the-lights")

links <- killTheLightsLink %>% 
  html_nodes('a[href*="lyrics"]') %>% 
  html_attr("href")

killLightsLyrics <- lapply(links, function(x) {
  songPage = read_html(x, encoding = "UTF-8") 
  
  lyrics = songPage %>% 
    html_nodes(".lyrics") %>% 
    html_text() %>% 
    str_replace_all(., "\n", " ") %>% 
    str_replace_all(., "\\[(.*?)\\]", "") %>% 
    str_squish(.) %>% 
    gsub("([a-z])([A-Z])", "\\1 \\2", .)
  
  title = songPage %>% 
    html_nodes("title") %>% 
    html_text() %>% 
    stringr::str_extract(., "(?=\\s\\W\\s).*(?<=\\s\\W\\s)")
  
  res = data.frame(lyrics = lyrics, 
                   title = title, 
                   stringsAsFactors = FALSE)
  
  return(res)
})

killLightsLyrics <- do.call("rbind", killLightsLyrics)

```

If you care to, you can look at the lyrics for each song (I don't recommend it).

Now, we can go through our tokenizing, stopword filtering, and correlation generation again:

```{r}
albumCors <- unnest_tokens(killLightsLyrics, words, lyrics) %>% 
  filter(!(.$words %in% stop_words$word)) %>% 
  pairwise_cor(., words, title, sort = TRUE)

rmarkdown::paged_table(albumCors)
```

These correlations are far more interesting! 

Out of pure curiousity, what is correlated with the word *coffee*?

```{r}
albumCors[albumCors$item1 == "coffee", ] %>% 
  rmarkdown::paged_table()
```
