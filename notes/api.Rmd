---
title: "APIs"
description: |
  Your Path To Easy Living
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


## Packages

You will need the following packages:

```{r}
library(httr2)

library(rvest)
```

## APIs

Whether you know it or not, you use application programming interfaces (APIs) all the time...how else would you be able to copy from one program and paste into another? While APIs exist at all levels of our interactions with computers, we are going to focus on using APIs to acquire data from providers. You will see APIs offered from most of the big tech companies (Facebook, Google, Reddit, just to name a small fraction), but not all of them are immediately useful for collecting data. 

### An Easy Example

The most basic of all APIs will allow us to pass information into a URL string and return the request:

A common task to encounter is to take a city and return the coordinates. We will check out more elaborate ways of performing this task with Google, but Zippopotam.us gives us something really easy:

```{r}
library(httr2)

request("https://api.zippopotam.us/us/61949") %>%
  req_perform()
```

All we need to do to construct this API call to GET is to append a zip code at the end of the link. Super easy and as easy as an API can get.

We see that we got a 200 status, but not much else. We need the data out of this request, which leads us, undoubtedly, to JavaScript Object Notation (JSON). Let's see how this works: 

```{r}
zipRequest <- request("https://api.zippopotam.us/us/il/redmon") %>%
  req_perform()

str(zipRequest)

resp_body_raw(zipRequest)
```

This is the raw request in byte codes -- no thanks.

```{r}
resp_body_string(zipRequest)
```

Depending upon the situation, you might need to return the text output and wrangle it yourself. Fortunately, we don't need to do that.

```{r}
resp_body_json(zipRequest)
```

The majority of APIs are going to return JSON objects (not all, but definitely most). Why? Mostly because it is the best way to deal with data on the web. It makes our life just a little bit harder, but it is not the worst thing that we will encounter. It does become difficult, though, when the JSON is malformed. 

That is a little clunky, so let's look at another alternative:

```{r}
zipText <- resp_body_string(zipRequest)

jsonlite::fromJSON(zipText, simplifyDataFrame = TRUE)
```

We are getting something closer to usable data, so we can just wrangle this into place. Let's just cheat and use `purrr`:

```{r}
purrr::flatten_df(jsonlite::fromJSON(zipText, simplifyDataFrame = TRUE)$places)
```

Depending upon how deeply nested your JSON return is, this will work just fine. If you have a deeply-nested JSON return, you will have to do some programmatic work.

```{python}
import json
import pandas as pd
import requests

pd.set_option('display.max_columns', None)

r = requests.get("http://api.zippopotam.us/us/il/redmon")

r.text

raw_text = r.text

json.loads(raw_text)

all_info = pd.read_json(raw_text)

pd.json_normalize(data = json.loads(raw_text), 
  record_path = 'places', 
  meta = ['country abbreviation', 'state abbreviation'])
```

### The Easy Road

Would you like to see how old a computer thinks you are based upon your name alone?

Try out the agify API!

```{r, eval = FALSE}
base_link <- "https://api.agify.io/?name="

your_name <- ""

complete_link <- paste0(base_link, your_name)
```


```{python, eval = FALSE}
base_link = "https://api.agify.io/?name="

your_name = ""

complete_link = base_link + your_name
```

### The Hard Road

If you want a challenge, see how many stars are in your sphere of causality (lightcone). If a single point of light was emitted at the time of your birth, how many stars has that light reached? This is a bit of a challenge, because you will need to parse the XML from the request.

```{r, eval = FALSE}
link <- "http://interconnected.org/more/lightcone/rss/?d=YYYY-MM-DD"
```

If you want to try this in R, you can do it pretty easily with the `XML` package, by first parsing and then taking it to a data frame:

```{r, eval = FALSE}
XML::xmlParse()

XML::xmlToDataFrame()
```

For python, you will need `requests` (again) and xml.etree.ElementTree. Unfortunately, it will require a loop. Here is a great <a href="https://stackabuse.com/reading-and-writing-xml-files-in-python-with-pandas/">resource</a> for tackling it without much hassle. If you use the page.text method, just remember that you have to read that string first (what is called xmlOut below):

```{python}
import xml.etree.ElementTree as ET

link = "http://interconnected.org/more/lightcone/rss/?d=1985-12-18"

page = requests.get(link)

xmlOut = ET.ElementTree(ET.fromstring(page.text))
```

No matter which of those roads you chose, we were dealing with an easy API -- not only are they simple, but they do not require a key. The more interesting APIs, however, will require you to pass a user key into the request. Usually, you can just pop it into the url request. 

We can use AlphaVantage as a good testing place. You can get a key here: https://www.alphavantage.co/support/#api-key

Before we start on that, there are a few things to remember. 

1. Always search for the "developers" tab for any given site -- they probably have an API.

2. Always read through the documentation. 

3. Look for example calls.

Once you have a key, you can explore the various endpoints that AlphaVantage has to offer. For demonstration, we will just use the time series weekly endpoint and have an interval set to 5 minutes.

```{r}
av_K <- "W2J61LUSTTXBV64M"

av_symbol <- "GOOGL"

av_link <- glue::glue("https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol={av_symbol}&apikey={av_K}")

av_request <- request(av_link) %>%
  req_perform() %>%
  resp_body_string()

av_read <- RJSONIO::fromJSON(av_request)

av_series <- av_read$`Weekly Time Series`

av_meta <- av_read$`Meta Data`

av_dates <- names(av_series)

return_wrangle <- function(x) {
  
  outDate <- purrr::flatten_df(av_series[x])
  
  outDate$Date <- x
  
  return(outDate)
}

av_output <- purrr::map_df(av_dates, ~return_wrangle(.x))

head(av_output)
```


```{python}
av_k = "W2J61LUSTTXBV64M"

av_symbol = "GOOGL"

av_link = "https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY&symbol={}&apikey={}".format(
  av_symbol, av_k)

av_request = requests.get(av_link)

av_json = av_request.json()

series_data = av_json['Weekly Time Series']

meta_data = av_json['Meta Data']

av_data = pd.DataFrame.from_dict(series_data, orient = 'index')

av_data['symbol'] = meta_data['2. Symbol']

av_data.reset_index(inplace = True)

av_data = av_data.rename(columns = {'index':'date'})
```

Now that you have some idea about how to construct an API call, turn your attention to the *Intraday* endpoint. It has a few pieces of required information: function, symbol, key, and interval (which we haven't seen yet). All you need to do is to add another bit of information to your url: `&interval=5min`. You should also notice that we have options on our `datatype` object, so we can see how that might work. 

```{r}
base_link <- "https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol="

av_link <- paste(base_link, 
                         av_symbol, "&interval=60min", "&datatype=csv",
                         "&apikey=", av_K, 
                         sep = "")

av_request <- request(av_link) %>%
  req_perform() %>%
  resp_body_string()

av_parsed <- read.delim(text = av_request, sep = ",")
```

Revise this to return more points -- you can focus on `outputsize` and `interval.`

There is a lot of cool information in this API, but as you continue messing with this, you will eventually find that you run into limitations. Those limitations might be related to the number of calls that you can make (there is a cap) or the data available to you (this one is actually pretty deep historically). This tends to be where many APIs will turn towards a subscription-based API -- the more money you pay, the more data you get. Twitter, for instance, was once very open. As time goes on, though, they are reducing their free access for normal clients (academics get some expanded goodness). 

There are APIs, though, that do not really limit you. Let's look at the Census API. It has everything you can imagine (free geocoding -- oh yeah), but it will make you work for it. 

```{r}
oldHouse <- request("https://geocoding.geo.census.gov/geocoder/locations/address?street=107+East+Street&city=redmon&state=IL&zip=61949&benchmark=Public_AR_Current&format=json") %>%
  req_perform() %>%
  resp_body_json()

oldHouse
```

Excellent! And easy. However, getting into the census API is really difficult, mostly because you need to know a lot about the census. If you need to use the census, I would just use the censusapi R package. 

Now that we can do a single call, let's look at some batch geocoding. We need to dig into the documentation just a little bit: https://geocoding.geo.census.gov/geocoder/Geocoding_Services_API.html

It might not be obvious what we would need to do, but we are going to need to utilize a different method other than GET. While we might be trying to get something from them, we need to POST something to their server.

Before we post anything, though, we see that we will need a csv file with specific information -- we can handle that:

```{r}
addresses <- data.frame(ID = c(1, 2), 
                        Address = c("104 East Street", "210 East Elizabeth"), 
                        City = c("Redmon", "Paris"), 
                        State = c("IL", "IL"), 
                        ZIP = c(61949, 61944))

write.table(addresses, "./addresses.csv", # Change this to your own machine!
            row.names = FALSE, col.names = FALSE, sep = ",")
```

Let's see what we can put together from the documentation. We have the endpoint, but that endpoint is going to need some additional information: benchmark and addressFile. Unlike what we covered last time, we can't just append those to the url; instead, we have to submit that information in the body or the head of the request.

Let's go with what the documentation tells us and try something out. Since the documentation isn't super clear, we can just do a quick test:

```{r}
batchEndpoint <- "https://geocoding.geo.census.gov/geocoder/locations/addressbatch"

request(batchEndpoint) %>%
  req_body_multipart(addressFile = httr::upload_file("./addresses.csv"), 
              benchmark = "DatasetType_Public_AR_Current") %>%
  req_perform(verbosity = 3)
```


While testing, I would recommend that you keep that verbose function in -- it will be a good start to telling you what might be going wrong.

Our Status: 400 means that it is a bad request and we can work with that. Let's think back to the documentation, read some more, and try something else out:

```{r}
request(batchEndpoint) %>%
  req_body_multipart(addressFile = httr::upload_file("./addresses.csv")) %>%
  req_headers("benchmark" = "Public_AR_Current") %>%
  req_perform(verbosity = 2)
```

Let's try this out now:

```{r}
geocodedAddresses <- request(batchEndpoint) %>%
  req_body_multipart(addressFile = httr::upload_file("./addresses.csv"), 
              benchmark = "Public_AR_Current") %>%
  req_perform(verbosity = 2)

read.delim(text = resp_body_string(geocodedAddresses), 
           sep = ",", 
           header = FALSE)
```

For the python people at the party:

```{python}
import requests
from io import StringIO

url = 'https://geocoding.geo.census.gov/geocoder/locations/addressbatch'

file = {'addressFile': ('address.csv', open('./addresses.csv', 'rb'), 'text/csv')}

data = {'benchmark':'Public_AR_Current'} 
  
r = requests.post(url = url, data = data, files = file) 

pd.read_csv(StringIO(r.text))
```

Using any POST request is just a minor variant of what we just saw. Let's tackle something fun. We had previously used GET to pass parameters into the url, but some APIs just want query parameters. 

```{r}
nbaPlayers <- request("https://www.balldontlie.io/api/v1/players") |> 
  req_url_query(page = 1, 
                per_page = 100) |> 
  req_perform()

nbaPlayers <- resp_body_string(nbaPlayers)

nbaPlayers <- jsonlite::fromJSON(nbaPlayers, flatten = TRUE)

nbaPlayers$meta
```

This is something that you see with a great number of APIs -- you will get data (what we want) and meta information (what we need to know). We need to use that information to get everything that we need.

Any ideas about how we can process everything? There are many ways, but let's see a few.

```{r, eval = FALSE}
nbaPlayersOut <- purrr::map_df(1:2, ~{
  nbaPlayers <- request("https://www.balldontlie.io/api/v1/players") |> 
    req_url_query(page = .x, 
                  per_page = 100) |> 
    req_perform()
  
  nbaPlayers <- resp_body_string(nbaPlayers)
  
  nbaPlayers <- jsonlite::fromJSON(nbaPlayers, flatten = TRUE)
  
  nbaPlayers$data
})
```

We could also use a handy `while` loop:

```{r, eval = FALSE}
pages <- 1

results <- list()

while(is.null(pages) == FALSE) {
  
  nbaPlayers <- request("https://www.balldontlie.io/api/v1/players") |> 
    req_url_query(page = pages, 
                  per_page = 100) |> 
    req_perform()
  
  nbaPlayers <- resp_body_string(nbaPlayers)
  
  nbaPlayers <- jsonlite::fromJSON(nbaPlayers, flatten = TRUE)
  
  results[[pages]] <- nbaPlayers$data
  
  if(is.null(nbaPlayers$meta$next_page) == FALSE) {
    pages <- pages + 1
  } else {
    pages <- NULL
  }
}

```

Something to make note of here is the rate at which you make requests. The API we were just hitting is free and unlimited -- probably one of those situations where we need to be cool with people who are cool with us. Let's put a little bit of a time out in there:

```{r, eval = FALSE}
pages <- 1

results <- list()

while(is.null(pages) == FALSE) {
  Sys.sleep(sample(runif(1, min = 1, max = 3)))
  
   nbaPlayers <- request("https://www.balldontlie.io/api/v1/players") |> 
    req_url_query(page = pages, 
                  per_page = 100) |> 
    req_perform()
  
  nbaPlayers <- resp_body_string(nbaPlayers)
  
  nbaPlayers <- jsonlite::fromJSON(nbaPlayers, flatten = TRUE)
  
  results[[pages]] <- nbaPlayers$data
  
  if(is.null(nbaPlayers$meta$next_page) == FALSE) {
    pages <- pages + 1
  } else {
    pages <- NULL
  }
}

```

Even with this time out, the multi-step approach is something that you should expect to see. It can take this pagination form or it can take finding specific information from one API call to pass along to another API call.

```{python}
import numpy
import time

page = 1

result = pd.DataFrame()

while page != None:
  
  time.sleep(numpy.random.uniform(low = .5, high = 1.5))

  params = {'page': page, 
    'per_page': 100}
  
  nba_players = requests.get('https://www.balldontlie.io/api/v1/players', 
    params = params)
  
  nba_json = nba_players.json()
  
  nba_data = nba_json['data']
  
  result = result.append(pd.json_normalize(nba_data, meta = ['id']))
  
  page = nba_json['meta']['next_page']

```

We probably know that Youtube comments are largely insane, so let's see what we might be able to get from the song with a huge number of comments: DNA. 

```{r}
songSearchText <- "BTS%20DNA"

k <- "AIzaSyCTyTAFxB7PSy0vKB4Dj58w6v3bPQhmPTU"
    
callConstruction <- paste("https://www.googleapis.com/youtube/v3/search?part=id,snippet&q=", 
                          songSearchText, 
                          "&maxResults=1&type=video&key=", k,
                          sep = "")

video_id_req <- request(callConstruction) |> 
  req_perform() |> 
  resp_body_json()

videoID <- video_id_req$items[[1]]$id$videoId

commentLink <- paste("https://www.googleapis.com/youtube/v3/commentThreads?part=id&videoId=", 
                     videoID, "&key=", k, 
                     sep = "")

commentReturn <- request(commentLink) |> 
  req_perform()

commentData <- jsonlite::fromJSON(resp_body_string(commentReturn))

commentData
```

A lot of stuff to talk about here. First, you will notice that we didn't actually get any comments -- we just got IDs for comments. You can probably guess what we need to do next. Before that, you should also notice that we get a `nextPageToken` -- we can pass that as a parameter into our request to get the next page of comments.

```{r}
commentID <- commentData$items$id[1]

commentTextLink <- paste("https://www.googleapis.com/youtube/v3/comments?part=id,snippet&id=", 
                     commentID, "&key=", k, 
                     "&textFormat=plainText",
                     sep = "")

request(commentTextLink) |> 
  req_perform() |> 
  resp_body_string() |> 
  jsonlite::fromJSON()
```

Let's see what this might look like with multiple comments:

```{r}
commentID <- commentData$items$id[1:3]

idString <- paste("&id=", commentID, collapse = "", sep = "")

commentTextLink <- paste("https://www.googleapis.com/youtube/v3/comments?part=id,snippet", 
                              idString, 
                              "&key=", k, "&textFormat=plainText",
                              sep = "")

commentReturn <- request(commentTextLink) |> 
  req_perform() |> 
  resp_body_string() |> 
  jsonlite::fromJSON()

commentReturn
```
