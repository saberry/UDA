---
title: "Modern I/O"
description: |
  Moving Beyond Commas
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Javascript Object Notation

The web runs on data stored in Javascript Object Notation (JSON). Every single table with dynamic updates is done through JSON. Website make requests to each other with JSON. It is with great sadness that I present to you JSON as the first major topic in Unstructured. Let's take a look at some <a href="https://www.json.org/json-en.html">JSON's</a> example of JSON and see if you can figure out why this might make me sad.

```
{
    "glossary": {
        "title": "example glossary",
		"GlossDiv": {
            "title": "S",
			"GlossList": {
                "GlossEntry": {
                    "ID": "SGML",
					"SortAs": "SGML",
					"GlossTerm": "Standard Generalized Markup Language",
					"Acronym": "SGML",
					"Abbrev": "ISO 8879:1986",
					"GlossDef": {
                        "para": "A meta-markup language, used to create markup languages such as DocBook.",
						"GlossSeeAlso": ["GML", "XML"]
                    },
					"GlossSee": "markup"
                }
            }
        }
    }
}
```

So structured...so incredibly structured. However, that structure does not mean it is ready for us. Let's see what we can do with the `critters` file on Canvas.

```{r}
library(jsonlite) # You will need to install this.

congress_critters <- fromJSON("~/Documents/UDA/data/critters.json")

dplyr::glimpse(congress_critters$objects)
```

You could also use `read_json`, but the `fromJSON` function will try to give you a nicer return (which it usually does). Let's see how we might tackle something a bit rougher:

```{r}
congress_hard <- read_json("~/Documents/UDA/data/critters.json")
```


```{r}
congress_brief <- purrr::map_df(1:100, ~{
  data.frame(id = congress_hard$objects[[.x]]$person$bioguideid, 
             link = congress_hard$objects[[.x]]$person$link, 
             state = congress_hard$objects[[.x]]$state)
})

head(congress_brief)
```


```{python}
import json # Already comes with python.
import pandas as pd
from datetime import date

critters_json = json.load(open('/Users/sethberry/Documents/UDA/data/critters.json'))

critters = pd.json_normalize(critters_json, record_path = ['objects'])

critters['age'] = pd.to_datetime(date.today()) - pd.to_datetime(critters['person.birthday'])

critters['age'] = critters['age'].dt.days / 365
```

Just like R, pandas will offer different methods for reading json. If you don't have anything too nesty, you can just use `pd.read_json`. 

You've probably got the general idea. No matter the situation, you are going to need to put in some effort to get the data in the right shape.

## Python

Like R, Python has undergone an evolution. If you want to work hard, everything could be done with numpy (I'd rather let all of you hit me in the kneecaps). People wanted/needed something a little easier, so pandas became popular. Talk to anybody about speed, though, and you will hear complaints about pandas being slow. Let's explore a few different packages: dask and polars

### pandas

No doubt that pandas is popular; instead of reinventing the wheel people have decided to leverage pandas's API. With pandas 2.0, we saw big boosts with the arrow backend!

```{python}
import pandas as pd
import pyarrow as pa
import re
import timeit

billboard_songs_pa = pd.read_csv(
  '/Users/sethberry/Documents/UDA/data/billboard_songs_11_23.csv', 
  engine='pyarrow', dtype_backend='pyarrow'
)

billboard_songs = pd.read_csv(
  '/Users/sethberry/Documents/UDA/data/billboard_songs_11_23.csv', 
)

setup_normal = '''
import pandas as pd
billboard_songs = pd.read_csv(
  '/Users/sethberry/Documents/UDA/data/billboard_songs_11_23.csv', 
)
'''

setup_pyarrow = '''
import pandas as pd
import pyarrow as pa
billboard_songs = pd.read_csv(
  '/Users/sethberry/Documents/UDA/data/billboard_songs_11_23.csv', 
  engine='pyarrow', dtype_backend='pyarrow'
)
'''

statement = '''
billboard_songs['lyrics'].str.replace(
  'and',
  '&', 
  regex=True
)
'''

tests = {'setup_normal': setup_normal, 'setup_pyarrow': setup_pyarrow}

for key, value in tests.items():
  times = timeit.repeat(setup=value, 
                        stmt=statement,
                        repeat=2,
                        number=10
                        )
  print('The lowest time for ' + key + ': ', min(times))
  
```

### polars

```{python}
import polars as pl

billboard_songs = pl.read_csv(
  '/Users/sethberry/Documents/UDA/data/billboard_songs_11_23.csv'
)

billboard_songs.schema

billboard_songs['week'].head()

billboard_songs = billboard_songs.with_columns(
   pl.col("week").str.to_date("%Y-%m-%d", strict=False)
)
  
billboard_songs['week'].head()  

billboard_songs.group_by('genre').count()

lazy_songs = billboard_songs.lazy()

lazy_songs.group_by('genre').count().collect()
```


### Who's Faster?

Let's time them out!

```{python}
setup_normal = '''
import pandas as pd
billboard_songs = pd.read_csv(
  '/Users/sethberry/Documents/UDA/data/billboard_songs_11_23.csv', 
)
'''

setup_pyarrow = '''
import pandas as pd
import pyarrow as pa
billboard_songs = pd.read_csv(
  '/Users/sethberry/Documents/UDA/data/billboard_songs_11_23.csv', 
  engine='pyarrow', dtype_backend='pyarrow'
)
'''

setup_polars = '''
import polars as pl
billboard_songs = pl.read_csv(
  '/Users/sethberry/Documents/UDA/data/billboard_songs_11_23.csv'
)
'''

# Works for polars, but is deprecated!
statement = '''
billboard_songs.groupby('genre').count()
'''

tests = {
  'setup_normal': setup_normal, 
  'setup_pyarrow': setup_pyarrow, 
  'setup_polars': setup_polars
}

for key, value in tests.items():
  times = timeit.repeat(setup=value, 
                        stmt=statement,
                        repeat=10,
                        number=10
                        )
  print('The lowest time for ' + key + ': ', min(times))

```